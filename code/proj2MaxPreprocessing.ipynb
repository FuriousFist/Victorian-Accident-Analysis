{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ACCIDENT_NO NODE_TYPE  DEG_URBAN_NAME  POSTCODE_CRASH ACCIDENT_TIME  \\\n",
      "0       T20120000009         N  RURAL_VICTORIA            3981      02:25:00   \n",
      "1       T20120000012         N      MELB_URBAN            3170      02:00:00   \n",
      "2       T20120000013         I      MELB_URBAN            3169      03:35:00   \n",
      "3       T20120000018         I  RURAL_VICTORIA            3505      05:15:00   \n",
      "4       T20120000021         N      MELB_URBAN            3942      07:30:00   \n",
      "...              ...       ...             ...             ...           ...   \n",
      "178603  T20250000840         N      MELB_URBAN            3054      18:50:00   \n",
      "178604  T20250001075         N      MELB_URBAN            3022      16:30:00   \n",
      "178605  T20250001671         I      MELB_URBAN            3073      09:11:00   \n",
      "178606  T20250001949         N    SMALL_CITIES            3212      07:28:00   \n",
      "178607  T20250002210         I     SMALL_TOWNS            3809      09:20:00   \n",
      "\n",
      "        DAY_OF_WEEK  LIGHT_CONDITION  ROAD_GEOMETRY  SPEED_ZONE ROAD_TYPE  \\\n",
      "0                 1                5              5         100      ROAD   \n",
      "1                 1                3              1          80      ROAD   \n",
      "2                 1                3              2          60      ROAD   \n",
      "3                 1                5              1         100   HIGHWAY   \n",
      "4                 1                1              5          50      ROAD   \n",
      "...             ...              ...            ...         ...       ...   \n",
      "178603            3                2              5         999    STREET   \n",
      "178604            2                2              5         100      ROAD   \n",
      "178605            2                1              2         999      ROAD   \n",
      "178606            4                1              5         100   HIGHWAY   \n",
      "178607            2                1              1          80   HIGHWAY   \n",
      "\n",
      "       ATMOSPH_COND SURFACE_COND  SEVERITY  \n",
      "0               [1]          [1]         3  \n",
      "1               [1]          [1]         2  \n",
      "2               [1]          [1]         2  \n",
      "3               [1]          [1]         3  \n",
      "4               [1]          [1]         3  \n",
      "...             ...          ...       ...  \n",
      "178603          [9]          [1]         3  \n",
      "178604          [1]          [1]         3  \n",
      "178605          [1]          [1]         3  \n",
      "178606          [1]          [1]         3  \n",
      "178607          [1]          [1]         3  \n",
      "\n",
      "[178608 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "############################################# Utilised datasets\n",
    "#These are all 'event' factors, they are the same for every accident.\n",
    "# Accident_location.csv:\n",
    "# Accident.csv:\n",
    "# Atmospheric_cond.csv:\n",
    "# Node.csv:\n",
    "# Road_surface_cond.csv:\n",
    "\n",
    "\n",
    "############################################# Mappings\n",
    "# These are simply value mappings for the categorical data, just for reference.\n",
    "\n",
    "node_type_from_id = {\n",
    "    \"I\": \"Intersection\",\n",
    "    \"N\": \"Non-Intersection\",\n",
    "    \"O\": \"Off Road\",\n",
    "    \"U\": \"Unknown\"\n",
    "}\n",
    "\n",
    "light_condition_from_id = {\n",
    "    1: \"Day\",\n",
    "    2: \"Dusk/dawn\",\n",
    "    3: \"Dark street lights on\",\n",
    "    4: \"Dark street lights off\",\n",
    "    5: \"Dark no street lights\",\n",
    "    6: \"Dark street lights unknown\",\n",
    "    9: \"Unknown\"\n",
    "}\n",
    "\n",
    "road_geometry_from_id = {\n",
    "    1: \"Cross intersection\",\n",
    "    2: \"T Intersection\",\n",
    "    3: \"Y Intersection\",\n",
    "    4: \"Multiple intersections\",\n",
    "    5: \"Not at intersection\",\n",
    "    6: \"Dead end\",\n",
    "    7: \"Road closure\",\n",
    "    8: \"Private property\",\n",
    "    9: \"Unknown\"\n",
    "}\n",
    "\n",
    "atmospheric_cond_from_id = {\n",
    "    1: \"Clear\",\n",
    "    2: \"Raining\",\n",
    "    3: \"Snowing\",\n",
    "    4: \"Fog\",\n",
    "    5: \"Smoke\",\n",
    "    6: \"Dust\",\n",
    "    7: \"Strong winds\",\n",
    "    9: \"Not known\"\n",
    "}\n",
    "\n",
    "surface_cond_from_id = {\n",
    "    1: \"Dry\",\n",
    "    2: \"Wet\",\n",
    "    3: \"Muddy\",\n",
    "    4: \"Snowy\",\n",
    "    5: \"Icy\",\n",
    "    9: \"Unknown\"\n",
    "}\n",
    "\n",
    "day_from_id = {\n",
    "    1: \"Sunday\",\n",
    "    2: \"Monday\",\n",
    "    3: \"Tuesday\",\n",
    "    4: \"Wednesday\",\n",
    "    5: \"Thursday\",\n",
    "    6: \"Friday\",\n",
    "    7: \"Saturday\"\n",
    "}\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "\n",
    "\n",
    "def event_preprocess():\n",
    "\n",
    "    #181055 rows/accidents, keeping only important columns.\n",
    "    accident_df = pd.read_csv(\"/Users/armaansidhu/Documents/Uni/year-2/EODP/code/proj_2/datasets/accident.csv\")\n",
    "    COLS_TO_KEEP = [\"ACCIDENT_NO\", \"ACCIDENT_TIME\", \"DAY_OF_WEEK\", \"LIGHT_CONDITION\", \"ROAD_GEOMETRY\", \"SPEED_ZONE\", \"SEVERITY\"]\n",
    "    accident_df = accident_df[COLS_TO_KEEP]\n",
    "\n",
    "\n",
    "    #181055 rows/accidents, again keeping only important columns.\n",
    "    accident_location_df = pd.read_csv(\"/Users/armaansidhu/Documents/Uni/year-2/EODP/code/proj_2/datasets/accident_location.csv\")\n",
    "    COLS_TO_KEEP = [\"ACCIDENT_NO\", \"ROAD_TYPE\"]\n",
    "    accident_location_df = accident_location_df[COLS_TO_KEEP]\n",
    "\n",
    "\n",
    "    #183405 rows/accidents before pre-processing. This is because the same accident can have multiple atmospheric conditions. Fixed now to 181055 rows, with ATMOSPH_COND now containing a list of atmospheric conditions for each accident.\n",
    "    atmospheric_cond_df = pd.read_csv(\"/Users/armaansidhu/Documents/Uni/year-2/EODP/code/proj_2/datasets/atmospheric_cond.csv\")\n",
    "    filtered_atmospheric_cond_df = atmospheric_cond_df.groupby('ACCIDENT_NO')['ATMOSPH_COND'].agg(list).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    #182062 rows/accidents for the same reason as the previous, fixed to be 181055 rows.\n",
    "    road_surface_cond_df = pd.read_csv(\"/Users/armaansidhu/Documents/Uni/year-2/EODP/code/proj_2/datasets/road_surface_cond.csv\")\n",
    "    filtered_road_surface_cond_df = road_surface_cond_df.groupby('ACCIDENT_NO')['SURFACE_COND'].agg(list).reset_index()\n",
    "\n",
    "\n",
    "    #183830 rows/accidents for the same reason as the previous ones. Its a little weird though, they are saying\n",
    "    #the same accident can have multiple locations which seems kind of stupid. \n",
    "    #NOTE: for now, I literally just take the first instance of location for each accident.\n",
    "    node_df = pd.read_csv(\"/Users/armaansidhu/Documents/Uni/year-2/EODP/code/proj_2/datasets/node.csv\")\n",
    "    columns_to_keep = ['ACCIDENT_NO', 'NODE_TYPE', 'DEG_URBAN_NAME', 'POSTCODE_CRASH']\n",
    "    filtered_node_df = node_df[columns_to_keep]\n",
    "\n",
    "\n",
    "    #Now we get 180970 rows which is less than the 183405 accidents. This means there are 85 accidents that are in all of the other dataframes but not in the node dataframe. I just remove these from the total dataframe for now, giving us a total of 180970 rows.\n",
    "    filtered_node_df = filtered_node_df.drop_duplicates(subset='ACCIDENT_NO')\n",
    "\n",
    "\n",
    "    #We copy from the node df that doesn't contain those 85 accidents, so all operations will leave out the bad 85 accidents.\n",
    "    event_df = filtered_node_df.copy()\n",
    "    event_df = event_df.merge(accident_df, on='ACCIDENT_NO', how='inner')\n",
    "    event_df = event_df.merge(accident_location_df, on='ACCIDENT_NO', how='inner')\n",
    "    event_df = event_df.merge(filtered_atmospheric_cond_df, on='ACCIDENT_NO', how='inner')\n",
    "    event_df = event_df.merge(filtered_road_surface_cond_df, on='ACCIDENT_NO', how='inner')\n",
    "\n",
    "    #Move the severity column to the end just for clarity:\n",
    "    event_df = event_df[[col for col in event_df.columns if col != 'SEVERITY'] + ['SEVERITY']]\n",
    "\n",
    "    #We have 180970 rows and 13 columns.\n",
    "    event_df.to_csv(\"event_df.csv\", index=False)\n",
    "\n",
    "    return event_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    event_df = event_preprocess()\n",
    "    print(event_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
